#1.Differential analysis with limma (with covariates)--------------------------------------------------
# Packages
suppressPackageStartupMessages({
  library(readr)       # or readxl if you use Excel
  library(dplyr)
  library(tidyr)
  library(tibble)
  library(limma)
  library(openxlsx)    # optional: write Excel
})
set.seed(007) 
# 2) Load data (REPLACE)
# Expression matrix: rows = features (proteins/genes), cols = samples
# Metadata: one row per sample; must include: sample_id, group, covariates
expr <- read_csv("expression_matrix.csv")          # first column = feature id
meta <- read_csv("metadata.csv")                   # columns: sample_id, group, sex, residence, agegrp, edugrp

# 3) Tidy expression: set rownames to feature id
stopifnot(ncol(expr) >= 2)
expr <- as.data.frame(expr)
feature_col <- 1
rownames(expr) <- expr[[feature_col]]
expr <- expr[, -(feature_col), drop = FALSE]

# 4) Align samples between expr and meta
stopifnot("sample_id" %in% colnames(meta))
common_ids <- intersect(colnames(expr), meta$sample_id)
expr <- expr[, common_ids, drop = FALSE]
meta <- meta %>% filter(sample_id %in% common_ids)
# Safety check
stopifnot(identical(colnames(expr), meta$sample_id))

# 5) Model design (EDIT HERE)
#    - group: two levels, e.g., "CI", "NCI"
#    - covariates: e.g., sex, residence, agegrp, edugrp
meta <- meta %>%
  mutate(
    group    = factor(group),            # ensure factor
    sex      = factor(sex),
    residence= factor(residence),
    agegrp   = factor(agegrp),
    edu      = factor(edugrp))
# Design with no intercept: each group gets a column
design <- model.matrix(~ 0 + group + sex + residence + agegrp + edugrp, data = meta)
colnames(design) <- make.names(colnames(design))

# 6) Fit linear model with limma
fit <- lmFit(expr, design)

# 7) Contrast: GroupB vs GroupA (EDIT the level names to match your meta$group)
#    If design has columns groupA and groupB:
stopifnot(all(c("groupA","groupB") %in% colnames(design)))
cont <- makeContrasts(BvsA = groupB - groupA, levels = design)
fit2 <- contrasts.fit(fit, cont)
fit2 <- eBayes(fit2)

# 8) Results (BH-adjusted p-values)
res <- topTable(fit2, coef = "BvsA", number = Inf, adjust.method = "BH") %>%
  rownames_to_column(var = "Feature") %>%
  rename(
    logFC   = logFC,     # effect size
    AveExpr = AveExpr,
    t       = t,
    P.Value = P.Value,
    adj.P.Val = adj.P.Val,
    B = B
  )


# 2.Trend clustering with Mfuzz (generic template)--------------------
# Goal: cluster mean expression profiles across ordered groups (e.g., BMI quartiles)
suppressPackageStartupMessages({
  library(dplyr); library(tidyr); library(tibble)
  library(readr)        # or readxl
  library(limma)        # optional
  library(Mfuzz)
  library(Biobase)
  library(vegan)        # for cascadeKM (optional, choose K)
})
# 1) Load your data
# expr: rows = features (proteins/genes), cols = samples; first column = feature id
# meta: one row per sample; columns include: sample_id, BMI_quartile (or any ordered group), and other covariates
# Example (replace with your own I/O):
# expr <- read_csv("expression_matrix.csv")            # first column: feature_id
# meta <- read_csv("metadata.csv")                     # sample_id, BMI_quartile

# 2) Prepare expression
stopifnot(ncol(expr) >= 2)
expr <- as.data.frame(expr)
rownames(expr) <- expr[[1]]
expr <- expr[, -1, drop = FALSE]         # numeric matrix: features x samples

# 3) Align samples with meta
stopifnot("sample_id" %in% colnames(meta))
common_ids <- intersect(colnames(expr), meta$sample_id)
expr <- expr[, common_ids, drop = FALSE]
meta <- meta %>% filter(sample_id %in% common_ids)
stopifnot(identical(colnames(expr), meta$sample_id))

# 4) Compute group means
# Use an ordered/ordinal grouping variable, e.g., BMI quartiles
stopifnot("BMI_quartile" %in% colnames(meta))
meta <- meta %>% mutate(BMI_quartile = factor(BMI_quartile, ordered = TRUE))
# Make a long table then compute mean per group
long_df <- expr %>%
  as.data.frame() %>%
  rownames_to_column("Feature") %>%
  pivot_longer(-Feature, names_to = "sample_id", values_to = "value") %>%
  left_join(meta %>% select(sample_id, BMI_quartile), by = "sample_id")
group_mean <- long_df %>%
  group_by(Feature, BMI_quartile) %>%
  summarise(mean_value = mean(value, na.rm = TRUE), .groups = "drop")
# Back to wide: rows = features, cols = groups (ordered)
data_matrix <- group_mean %>%
  pivot_wider(names_from = BMI_quartile, values_from = mean_value) %>%
  arrange(Feature) %>%
  column_to_rownames("Feature") %>%
  as.matrix()
# (Optional) ensure columns are in the natural order of the factor
data_matrix <- data_matrix[, levels(meta$BMI_quartile), drop = FALSE]


# 5) (Optional) choose K using cascadeKM
set.seed(7)
# cascadeKM expects rows as observations; here each row is a feature profile across groups -> OK
ckm <- cascadeKM(data_matrix, inf.gr = 2, sup.gr = 12, iter = 500)
# Pick K with max CH index
ch <- ckm$results[,"ch"] %||% ckm$results[,"Calinski"] %||% ckm$results[, ncol(ckm$results)]
# Fallback if column name differs; adjust to your ckm$results column names if needed.
K <- as.integer(ckm$nf[ which.max(ch) ])
message(sprintf("Auto-selected K by CH index: %d", K))

# 6) Build ExpressionSet & standardise
eset <- new("ExpressionSet", exprs = data_matrix)     # rows = features, cols = ordered groups
eset <- standardise(eset)                              # z-score per feature
m <- mestimate(eset)                                   # fuzzifier

# 7) Fuzzy c-means clustering
cl <- mfuzz(eset, c = K, m = m)

# 8) Plot clusters
# One PDF with multi-panel cluster profiles
pdf("mfuzz_clusters.pdf", width = 10, height = 8)
mfuzz.plot(eset, cl = cl, mfrow = c(ceiling(K/4), 4), time.labels = colnames(exprs(eset)),
           new.window = FALSE)
dev.off()

# 9) Export cluster membership
membership <- as.data.frame(cl$membership)              # membership score per cluster
colnames(membership) <- paste0("Cluster_", seq_len(K))
membership <- membership %>%
  rownames_to_column("Feature") %>%
  mutate(Assigned = apply(.[, -1, drop = FALSE], 1, which.max))  # hard assignment
readr::write_csv(membership, "mfuzz_membership.csv")

# (Optional) list of features per cluster
clusters_list <- split(membership$Feature, membership$Assigned)
# saveRDS(clusters_list, file = "mfuzz_clusters_list.rds")


#3.Selecting and Categorizing Trend Clusters --------------------------------------
# This script identifies clusters with clear upward/downward trends or fluctuation patterns,
# and further classifies the fluctuation intensity (mild / intermediate / strong).
# 3.2.1 Identify clusters with clear monotonic (upward/downward) trends
results <- list()
cor_threshold <- 0.7   # Spearman correlation threshold
for (i in 1:10) {
  cluster_centers <- get(paste0("c", i))$centers  # extract cluster centers (mean profiles)
  
  # Average slope (mean of consecutive differences)
  growth_rate <- apply(cluster_centers, 1, function(x) mean(diff(x)))
  
  # Upward trends: positive mean slope and strong positive correlation with group order
  selected_up <- which(
    growth_rate > 0 &
      apply(cluster_centers, 1, function(x)
        cor.test(1:length(x), x, method = "spearman")$estimate) > cor_threshold
  )
  
  # Downward trends: negative mean slope and strong negative correlation
  selected_down <- which(
    growth_rate < 0 &
      apply(cluster_centers, 1, function(x)
        cor.test(1:length(x), x, method = "spearman")$estimate) < -cor_threshold
  )
  
  # Store results
  results[[i]] <- data.frame(
    Cluster = paste0("c", i),
    Upward_Trends = paste(selected_up, collapse = ", "),
    Downward_Trends = paste(selected_down, collapse = ", ")
  )
}

final_results <- do.call(rbind, results)
print(final_results)

# 3.2.2 Compute difference values between adjacent groups
#     → used to detect fluctuating (non-monotonic) clusters
all_diff_values <- list()
for (i in 1:10) {
  cluster_centers <- get(paste0("c", i))$centers
  diff_values <- apply(cluster_centers, 1, function(x) diff(x))  # differences between consecutive groups
  all_diff_values[[paste0("c", i)]] <- diff_values
}

# Identify fluctuating categories (having both positive and negative deltas)
all_fluctuating_categories <- list()

for (i in 1:10) {
  diff_cluster <- all_diff_values[[paste0("c", i)]]
  fluctuating <- c()
  
  for (j in 1:ncol(diff_cluster)) {
    diffs <- diff_cluster[, j]
    if (any(diffs > 0) && any(diffs < 0)) {  # contains both up and down
      diff_sign_change <- diffs[which(diffs != min(diffs) & diffs != max(diffs))]
      remaining <- abs(diffs[diffs != diff_sign_change])
      
      # if the sign-changing value is smaller (weaker) than others → real fluctuation
      if (abs(diff_sign_change) < min(remaining)) {
        fluctuating <- c(fluctuating, j)
      }
    }
  }
  
  all_fluctuating_categories[[paste0("c", i)]] <- fluctuating
}

fluctuating_df <- do.call(rbind, lapply(names(all_fluctuating_categories), function(c) {
  data.frame(Cluster = c,
             Fluctuating_Categories = paste(all_fluctuating_categories[[c]], collapse = ", "))
}))
print(fluctuating_df)



# 3.2.3 Classify fluctuation intensity (mild / intermediate / strong)
mild_fluct <- list()
strong_fluct <- list()
intermediate_fluct <- list()

for (i in 1:10) {
  diff_cluster <- all_diff_values[[paste0("c", i)]]
  
  mild <- c(); strong <- c(); intermediate <- c()
  
  for (j in 1:ncol(diff_cluster)) {
    diffs <- diff_cluster[, j]
    
    # Only evaluate categories with sign reversal (both + and - present)
    if (any(diffs > 0) && any(diffs < 0)) {
      signs <- sign(diffs)
      
      # Case: single reversal (simple up-down or down-up pattern)
      if (length(unique(signs)) == 2) {
        reverse_index <- which(signs != signs[which.max(tabulate(match(signs, unique(signs))))])
        
        if (length(reverse_index) == 1) {
          reverse_value <- abs(diffs[reverse_index])
          other_values <- abs(diffs[-reverse_index])
          
          # classify by relative magnitude of reversal
          if (reverse_value < min(other_values)) {
            mild <- c(mild, colnames(diff_cluster)[j])
          } else if (reverse_value > max(other_values)) {
            strong <- c(strong, colnames(diff_cluster)[j])
          } else {
            intermediate <- c(intermediate, colnames(diff_cluster)[j])
          }
        }
      }
    }
  }
  
  mild_fluct[[paste0("c", i)]] <- mild
  strong_fluct[[paste0("c", i)]] <- strong
  intermediate_fluct[[paste0("c", i)]] <- intermediate
}

# Combine into unified summary
clusters <- paste0("c", 1:10)
combined_df <- data.frame(
  Cluster = clusters,
  Mild_Fluctuation = sapply(clusters, function(c) paste(mild_fluct[[c]], collapse = ", ")),
  Intermediate_Fluctuation = sapply(clusters, function(c) paste(intermediate_fluct[[c]], collapse = ", ")),
  Strong_Fluctuation = sapply(clusters, function(c) paste(strong_fluct[[c]], collapse = ", "))
)

print(combined_df)
openxlsx::write.xlsx(combined_df, "fluctuation_summary.xlsx")



# Summary
# • Section 3.2.1 → identifies monotonic trends (up or down) using Spearman correlation.
# • Section 3.2.2 → finds clusters showing fluctuation (non-monotonic) behavior.
# • Section 3.2.3 → further classifies the fluctuating clusters into mild/intermediate/strong.
# Output:
#   - final_results: clusters with strong monotonic trends
#   - fluctuating_df: clusters with sign changes (fluctuation)
#   - combined_df: categorized fluctuation intensities


